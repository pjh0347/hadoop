{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "- Support ANSI SQL:2003 and HiveQL\n",
    "- Run Queries\n",
    "    - Spark SQL CLI : `./bin/spark-sql`\n",
    "    - Spark SQL API : `spark.sql()`\n",
    "    - Spark Thrift JDBC/ODBC Server and Client : `./sbin/start-thriftserver.sh` `./bin/beeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import apache_access_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[4]\") \\\n",
    "                    .appName(\"spark sql tutorial\") \\\n",
    "                    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enableHiveSupport()\n",
    "- connectivity to a persistent Hive metastore\n",
    "- support for Hive serdes\n",
    "- Hive user-defined functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE_PATH = 'access.log'\n",
    "rdd = sc.textFile(LOG_FILE_PATH) \\\n",
    "        .map(apache_access_log.parse) \\\n",
    "        .filter(lambda x: x is not None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GlobalTempView (cross-session scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createGlobalTempView('global_temp_view')\n",
    "#df.createOrReplaceGlobalTempView('global_temp_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 701 Aug 27 12:38 ./derby.log\r\n",
      "\r\n",
      "./metastore_db:\r\n",
      "total 16\r\n",
      "-rw-r--r--   1 root root  608 Aug 19 16:45 README_DO_NOT_TOUCH_FILES.txt\r\n",
      "-rw-r--r--   1 root root   38 Aug 27 12:38 db.lck\r\n",
      "-rw-r--r--   1 root root    4 Aug 27 12:38 dbex.lck\r\n",
      "drwxr-xr-x   6 root root  192 Aug 19 16:45 log\r\n",
      "drwxr-xr-x 190 root root 6080 Aug 19 16:59 seg0\r\n",
      "-rw-r--r--   1 root root  898 Aug 19 16:45 service.properties\r\n",
      "drwxr-xr-x   2 root root   64 Aug 27 12:38 tmp\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./metastore_db ./derby.log\n",
    "# for Hive metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "|               agent|    date|           host|identity|method|protocol|referer|size|status|               time|            url|user|\n",
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "|Mozilla/5.0 (Wind...|20151212|109.169.248.247|       -|   GET|HTTP/1.1|      -|4263|   200|2015-12-12 17:25:11|/administrator/|   -|\n",
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from global_temp.global_temp_view limit 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = spark.newSession()\n",
    "assert(spark!=spark2)\n",
    "assert(spark.sparkContext==spark2.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "|               agent|    date|           host|identity|method|protocol|referer|size|status|               time|            url|user|\n",
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "|Mozilla/5.0 (Wind...|20151212|109.169.248.247|       -|   GET|HTTP/1.1|      -|4263|   200|2015-12-12 17:25:11|/administrator/|   -|\n",
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark2.sql(\"SELECT * FROM global_temp.global_temp_view limit 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.dropGlobalTempView('global_temp_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE GLOBAL TEMPORARY VIEW global_temp_view2 AS SELECT 1 as a, 2 as b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM global_temp.global_temp_view2').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TempView (current session scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView('temp_view')\n",
    "#df.createOrReplaceTempView(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "|               agent|    date|           host|identity|method|protocol|referer|size|status|               time|            url|user|\n",
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "|Mozilla/5.0 (Wind...|20151212|109.169.248.247|       -|   GET|HTTP/1.1|      -|4263|   200|2015-12-12 17:25:11|/administrator/|   -|\n",
      "+--------------------+--------+---------------+--------+------+--------+-------+----+------+-------------------+---------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from temp_view limit 1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark2.sql('select * from temp_view limit 1').show() # throw exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.dropTempView('temp_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(spark.catalog.isCached('temp_view')==False)\n",
    "\n",
    "spark.catalog.cacheTable('temp_view')\n",
    "assert(spark.catalog.isCached('temp_view')==True)\n",
    "\n",
    "spark.catalog.uncacheTable('temp_view')\n",
    "#spark.catalog.clearCache()\n",
    "assert(spark.catalog.isCached('temp_view')==False)\n",
    "\n",
    "spark.catalog.cacheTable('temp_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Databases, Tables and Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n",
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|     test|      false|\n",
      "|        |temp_view|       true|\n",
      "+--------+---------+-----------+\n",
      "\n",
      "+----------+\n",
      "|  function|\n",
      "+----------+\n",
      "|         !|\n",
      "|         %|\n",
      "|         &|\n",
      "|         *|\n",
      "|         +|\n",
      "|         -|\n",
      "|         /|\n",
      "|         <|\n",
      "|        <=|\n",
      "|       <=>|\n",
      "|         =|\n",
      "|        ==|\n",
      "|         >|\n",
      "|        >=|\n",
      "|         ^|\n",
      "|       abs|\n",
      "|      acos|\n",
      "|add_months|\n",
      "| aggregate|\n",
      "|       and|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print spark.catalog.listDatabases()\n",
    "#print spark.catalog.listTables()\n",
    "#print spark.catalog.listFunctions()\n",
    "spark.sql('show databases').show()\n",
    "spark.sql('show tables').show()\n",
    "spark.sql('show functions').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   20000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print df.count()\n",
    "spark.sql(\"select count(1) from temp_view\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### daily pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    date|count|\n",
      "+--------+-----+\n",
      "|20151212|  358|\n",
      "|20151213| 1361|\n",
      "|20151214|  996|\n",
      "|20151215|  975|\n",
      "|20151216| 1171|\n",
      "|20151217|  929|\n",
      "|20151218| 1307|\n",
      "|20151219| 2189|\n",
      "|20151220|  337|\n",
      "|20151221|  521|\n",
      "|20151222|  323|\n",
      "|20151223|  271|\n",
      "|20151224|  322|\n",
      "|20151225|  216|\n",
      "|20151226|  338|\n",
      "|20151227|  504|\n",
      "|20151228|  398|\n",
      "|20151229|  729|\n",
      "|20151230|  509|\n",
      "|20151231|  394|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.groupBy('date').count().sort('date')\n",
    "df2.cache()\n",
    "spark.sql(\"select date, count(1) as count from temp_view group by date order by date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    date|count|\n",
      "+--------+-----+\n",
      "|20151212|  358|\n",
      "|20151213| 1361|\n",
      "|20151214|  996|\n",
      "|20151215|  975|\n",
      "|20151216| 1171|\n",
      "|20151217|  929|\n",
      "|20151218| 1307|\n",
      "|20151219| 2189|\n",
      "|20151220|  337|\n",
      "|20151221|  521|\n",
      "|20151222|  323|\n",
      "|20151223|  271|\n",
      "|20151224|  322|\n",
      "|20151225|  216|\n",
      "|20151226|  338|\n",
      "|20151227|  504|\n",
      "|20151228|  398|\n",
      "|20151229|  729|\n",
      "|20151230|  509|\n",
      "|20151231|  394|\n",
      "+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### daily count of status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+\n",
      "|    date|status|count|\n",
      "+--------+------+-----+\n",
      "|20151212|   200|  355|\n",
      "|20151212|   404|    3|\n",
      "|20151213|   200| 1341|\n",
      "|20151213|   404|   18|\n",
      "|20151213|   500|    2|\n",
      "|20151214|   200|  990|\n",
      "|20151214|   404|    4|\n",
      "|20151214|   405|    1|\n",
      "|20151214|   500|    1|\n",
      "|20151215|   200|  962|\n",
      "|20151215|   301|    1|\n",
      "|20151215|   404|   10|\n",
      "|20151215|   500|    2|\n",
      "|20151216|   200| 1123|\n",
      "|20151216|   304|    1|\n",
      "|20151216|   404|   46|\n",
      "|20151216|   500|    1|\n",
      "|20151217|   200|  923|\n",
      "|20151217|   301|    1|\n",
      "|20151217|   304|    1|\n",
      "+--------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+------+-----+\n",
      "|    date|status|count|\n",
      "+--------+------+-----+\n",
      "|20151212|   200|  355|\n",
      "|20151212|   404|    3|\n",
      "|20151213|   200| 1341|\n",
      "|20151213|   404|   18|\n",
      "|20151213|   500|    2|\n",
      "|20151214|   200|  990|\n",
      "|20151214|   404|    4|\n",
      "|20151214|   405|    1|\n",
      "|20151214|   500|    1|\n",
      "|20151215|   200|  962|\n",
      "|20151215|   301|    1|\n",
      "|20151215|   404|   10|\n",
      "|20151215|   500|    2|\n",
      "|20151216|   200| 1123|\n",
      "|20151216|   304|    1|\n",
      "|20151216|   404|   46|\n",
      "|20151216|   500|    1|\n",
      "|20151217|   200|  923|\n",
      "|20151217|   301|    1|\n",
      "|20151217|   304|    1|\n",
      "+--------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('date', 'status').count().sort('date', 'status').show()\n",
    "spark.sql(\"select date, status, count(1) as count from temp_view group by date, status order by date, status\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### daily uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    date| uv|\n",
      "+--------+---+\n",
      "|20151212|120|\n",
      "|20151213|369|\n",
      "|20151214|319|\n",
      "|20151215|278|\n",
      "|20151216|311|\n",
      "|20151217|287|\n",
      "|20151218|210|\n",
      "|20151219| 54|\n",
      "|20151220| 70|\n",
      "|20151221| 69|\n",
      "|20151222| 65|\n",
      "|20151223| 85|\n",
      "|20151224| 87|\n",
      "|20151225| 56|\n",
      "|20151226| 39|\n",
      "|20151227| 38|\n",
      "|20151228| 40|\n",
      "|20151229| 40|\n",
      "|20151230| 36|\n",
      "|20151231| 38|\n",
      "+--------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---+\n",
      "|    date| uv|\n",
      "+--------+---+\n",
      "|20151212|120|\n",
      "|20151213|369|\n",
      "|20151214|319|\n",
      "|20151215|278|\n",
      "|20151216|311|\n",
      "|20151217|287|\n",
      "|20151218|210|\n",
      "|20151219| 54|\n",
      "|20151220| 70|\n",
      "|20151221| 69|\n",
      "|20151222| 65|\n",
      "|20151223| 85|\n",
      "|20151224| 87|\n",
      "|20151225| 56|\n",
      "|20151226| 39|\n",
      "|20151227| 38|\n",
      "|20151228| 40|\n",
      "|20151229| 40|\n",
      "|20151230| 36|\n",
      "|20151231| 38|\n",
      "+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('date').agg(F.countDistinct('host').alias('uv')).sort('date').show()\n",
    "spark.sql(\"select date, count(distinct host) as uv from temp_view group by date order by date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### daily uv (between 20151220 ~ 20151231)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    date| uv|\n",
      "+--------+---+\n",
      "|20151220| 70|\n",
      "|20151221| 69|\n",
      "|20151222| 65|\n",
      "|20151223| 85|\n",
      "|20151224| 87|\n",
      "|20151225| 56|\n",
      "|20151226| 39|\n",
      "|20151227| 38|\n",
      "|20151228| 40|\n",
      "|20151229| 40|\n",
      "|20151230| 36|\n",
      "|20151231| 38|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    date| uv|\n",
      "+--------+---+\n",
      "|20151220| 70|\n",
      "|20151221| 69|\n",
      "|20151222| 65|\n",
      "|20151223| 85|\n",
      "|20151224| 87|\n",
      "|20151225| 56|\n",
      "|20151226| 39|\n",
      "|20151227| 38|\n",
      "|20151228| 40|\n",
      "|20151229| 40|\n",
      "|20151230| 36|\n",
      "|20151231| 38|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['date'].between('20151220', '20151231')) \\\n",
    "  .groupBy('date').agg(F.countDistinct('host').alias('uv')).sort('date').show()\n",
    "\n",
    "spark.sql(\"select date, count(distinct host) as uv from temp_view \"\n",
    "          \"where date between '20151220' and '20151231' group by date order by date\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- *(1) Project\n",
      "         +- *(1) Filter (isnotnull(status#8) && (status#8 = 200))\n",
      "            +- InMemoryTableScan [status#8], [isnotnull(status#8), (status#8 = 200)]\n",
      "                  +- InMemoryRelation [agent#0, date#1, host#2, identity#3, method#4, protocol#5, referer#6, size#7L, status#8, time#9, url#10, user#11], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                        +- Scan ExistingRDD[agent#0,date#1,host#2,identity#3,method#4,protocol#5,referer#6,size#7L,status#8,time#9,url#10,user#11]\n"
     ]
    }
   ],
   "source": [
    "lines = spark.sql(\"explain select count(1) from temp_view where status = '200'\").collect()[0]['plan'].split('\\n')\n",
    "for line in lines:\n",
    "    print line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------+\n",
      "|url                     |get_length(url)|\n",
      "+------------------------+---------------+\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "+------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_length(s): return len(s)\n",
    "\n",
    "spark.udf.register('get_length', get_length)\n",
    "spark.sql(\"select url, get_length(url) from temp_view\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------+\n",
      "|url                     |get_length(url)|\n",
      "+------------------------+---------------+\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "|/administrator/         |15             |\n",
      "|/administrator/index.php|24             |\n",
      "+------------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_get_length = F.udf(get_length)\n",
    "df.select('url', udf_get_length('url')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "-- managed table\n",
    "CREATE TABLE tbl_apache_access_log ( \n",
    "  host STRING,\n",
    "  identity STRING,\n",
    "  remote_user STRING,\n",
    "  time TIMESTAMP,\n",
    "  method STRING,\n",
    "  url STRING,\n",
    "  protocol STRING,\n",
    "  status STRING,\n",
    "  size BIGINT,\n",
    "  referer STRING,\n",
    "  agent STRING)\n",
    "--PARTITIONED BY (p_time STRING, p_host STRING)\n",
    "STORED AS TEXTFILE\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x 2 root root 64 Aug 27 12:39 tbl_apache_access_log\r\n",
      "drwxr-xr-x 2 root root 64 Aug 19 16:49 test\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./spark-warehouse\n",
    "# for managed databases and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH 'access.log' INTO TABLE tbl_apache_access_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   20000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from tbl_apache_access_log\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP TABLE tbl_apache_access_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spark.apache.org/\n",
    "- https://spark.apache.org/docs/latest/api/sql/index.html\n",
    "- https://cwiki.apache.org/confluence/display/Hive/LanguageManual\n",
    "- Spark: The Definitive Guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
